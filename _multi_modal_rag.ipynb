{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "12bea030",
   "metadata": {},
   "source": [
    "# System Dependencies\n",
    "\n",
    "To get started with Unstructured.io, we need a few system-wide dependencies: \n",
    "\n",
    "## Poppler (poppler-utils)\n",
    "Handles PDF processing. It's a library that can extract text, images, and metadata from PDFs. Unstructured uses it to parse PDF documents and convert them into processable text.\n",
    "\n",
    "## Tesseract (tesseract-ocr) \n",
    "Optical Character Recognition (OCR) engine. When you have scanned documents, images with text, or PDFs that are essentially pictures, Tesseract reads the text from these images and converts it to machine-readable text.\n",
    "\n",
    "## libmagic\n",
    "File type detection library. It identifies what type of file you're dealing with (PDF, Word doc, image, etc.) by analyzing the file's content, not just the extension. This helps Unstructured choose the right processing method for each document."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "335f25b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linux\n",
    "# !apt-get install poppler-utils tesseract-ocr libmagic-dev\n",
    "\n",
    "# for mac\n",
    "!brew install poppler tesseract libmagic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fddda27d",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -Uq \"unstructured[all-docs]\" \n",
    "%pip install -Uq langchain_chroma \n",
    "%pip install -Uq langchain langchain-community langchain-openai \n",
    "%pip install -Uq python_dotenv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144884d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from typing import List\n",
    "\n",
    "# Unstructured for document parsing\n",
    "from unstructured.partition.pdf import partition_pdf\n",
    "from unstructured.chunking.title import chunk_by_title\n",
    "\n",
    "# LangChain components\n",
    "from langchain_core.documents import Document\n",
    "from langchain_openai import ChatOpenAI, OpenAIEmbeddings\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_core.messages import HumanMessage\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78fb6efb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def partition_document(file_path: str):\n",
    "    \"\"\"Extract elements from PDF using unstructured\"\"\"\n",
    "    print(f\"Partitioning document: {file_path}\")\n",
    "    \n",
    "    elements = partition_pdf(\n",
    "        filename=file_path,  # Path to your PDF file\n",
    "        strategy=\"hi_res\", # Use the most accurate (but slower) processing method of extraction\n",
    "        infer_table_structure=True, # Keep tables as structured HTML, not jumbled text\n",
    "        extract_image_block_types=[\"Image\"], # Grab images found in the PDF\n",
    "        extract_image_block_to_payload=True # Store images as base64 data you can actually use\n",
    "    )\n",
    "    \n",
    "    print(f\"Extracted {len(elements)} elements\")\n",
    "    return elements\n",
    "\n",
    "# Test with your PDF file\n",
    "file_path = \"./docs/attention-is-all-you-need.pdf\"  \n",
    "elements = partition_document(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2524980c",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements\n",
    "#len(elements)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "802f4cdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# All types of different atomic elements we see from unstructured\n",
    "set([str(type(el)) for el in elements])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9b3cd26",
   "metadata": {},
   "outputs": [],
   "source": [
    "elements[30].to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7abe8c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all images\n",
    "images = [element for element in elements if element.category == 'Image']\n",
    "print(f\"Found {len(images)} images\")\n",
    "\n",
    "images[0].to_dict()\n",
    "\n",
    "# Use https://codebeautify.org/base64-to-image-converter to view the base64 text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d9f175",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gather all table\n",
    "tables = [element for element in elements if element.category == 'Table']\n",
    "print(f\"Found {len(tables)} tables\")\n",
    "\n",
    "tables[0].to_dict()\n",
    "\n",
    "# Use https://jsfiddle.net/ to view the table html \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ecef416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_chunks_by_title(elements):\n",
    "    \"\"\"Create intelligent chunks using title-based strategy\"\"\"\n",
    "    print(\"ðŸ”¨ Creating smart chunks...\")\n",
    "    \n",
    "    chunks = chunk_by_title(\n",
    "        elements, # The parsed PDF elements from previous step\n",
    "        max_characters=3000, # Hard limit - never exceed 3000 characters per chunk\n",
    "        new_after_n_chars=2400, # Try to start a new chunk after 2400 characters\n",
    "        combine_text_under_n_chars=500 # Merge tiny chunks under 500 chars with neighbors\n",
    "    )\n",
    "    \n",
    "    print(f\"Created {len(chunks)} chunks\")\n",
    "    return chunks\n",
    "\n",
    "# Create chunks\n",
    "chunks = create_chunks_by_title(elements)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94189379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View all chunks\n",
    "# chunks\n",
    "\n",
    "# All unique types\n",
    "set([str(type(chunk)) for chunk in chunks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6410fa1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# View a single chunk\n",
    "# chunks[2].to_dict()\n",
    "\n",
    "# View original elements\n",
    "chunks[4].metadata.orig_elements\n",
    "# Note: 4th chunk has the first image + 11th chunk has the first table in the sample PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0967ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def separate_content_types(chunk):\n",
    "    \"\"\"Analyze what types of content are in a chunk\"\"\"\n",
    "    content_data = {\n",
    "        'text': chunk.text,\n",
    "        'tables': [],\n",
    "        'images': [],\n",
    "        'types': ['text']\n",
    "    }\n",
    "    \n",
    "    # Check for tables and images in original elements\n",
    "    if hasattr(chunk, 'metadata') and hasattr(chunk.metadata, 'orig_elements'):\n",
    "        for element in chunk.metadata.orig_elements:\n",
    "            element_type = type(element).__name__\n",
    "            \n",
    "            # Handle tables\n",
    "            if element_type == 'Table':\n",
    "                content_data['types'].append('table')\n",
    "                table_html = getattr(element.metadata, 'text_as_html', element.text)\n",
    "                content_data['tables'].append(table_html)\n",
    "            \n",
    "            # Handle images\n",
    "            elif element_type == 'Image':\n",
    "                if hasattr(element, 'metadata') and hasattr(element.metadata, 'image_base64'):\n",
    "                    content_data['types'].append('image')\n",
    "                    content_data['images'].append(element.metadata.image_base64)\n",
    "    \n",
    "    content_data['types'] = list(set(content_data['types']))\n",
    "    return content_data\n",
    "\n",
    "def create_ai_enhanced_summary(text: str, tables: List[str], images: List[str]) -> str:\n",
    "    \"\"\"Create AI-enhanced summary for mixed content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"You are creating a searchable description for document content retrieval.\n",
    "\n",
    "        CONTENT TO ANALYZE:\n",
    "        TEXT CONTENT:\n",
    "        {text}\n",
    "\n",
    "        \"\"\"\n",
    "        \n",
    "        # Add tables if present\n",
    "        if tables:\n",
    "            prompt_text += \"TABLES:\\n\"\n",
    "            for i, table in enumerate(tables):\n",
    "                prompt_text += f\"Table {i+1}:\\n{table}\\n\\n\"\n",
    "        \n",
    "                prompt_text += \"\"\"\n",
    "                YOUR TASK:\n",
    "                Generate a comprehensive, searchable description that covers:\n",
    "\n",
    "                1. Key facts, numbers, and data points from text and tables\n",
    "                2. Main topics and concepts discussed  \n",
    "                3. Questions this content could answer\n",
    "                4. Visual content analysis (charts, diagrams, patterns in images)\n",
    "                5. Alternative search terms users might use\n",
    "\n",
    "                Make it detailed and searchable - prioritize findability over brevity.\n",
    "\n",
    "                SEARCHABLE DESCRIPTION:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add images to the message\n",
    "        for image_base64 in images:\n",
    "            message_content.append({\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "            })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"      AI summary failed: {e}\")\n",
    "        # Fallback to simple summary\n",
    "        summary = f\"{text[:300]}...\"\n",
    "        if tables:\n",
    "            summary += f\" [Contains {len(tables)} table(s)]\"\n",
    "        if images:\n",
    "            summary += f\" [Contains {len(images)} image(s)]\"\n",
    "        return summary\n",
    "\n",
    "def summarise_chunks(chunks):\n",
    "    \"\"\"Process all chunks with AI Summaries\"\"\"\n",
    "    print(\" Processing chunks with AI Summaries...\")\n",
    "    \n",
    "    langchain_documents = []\n",
    "    total_chunks = len(chunks)\n",
    "    \n",
    "    for i, chunk in enumerate(chunks):\n",
    "        current_chunk = i + 1\n",
    "        print(f\"   Processing chunk {current_chunk}/{total_chunks}\")\n",
    "        \n",
    "        # Analyze chunk content\n",
    "        content_data = separate_content_types(chunk)\n",
    "        \n",
    "        # Debug prints\n",
    "        print(f\"     Types found: {content_data['types']}\")\n",
    "        print(f\"     Tables: {len(content_data['tables'])}, Images: {len(content_data['images'])}\")\n",
    "        \n",
    "        # Create AI-enhanced summary if chunk has tables/images\n",
    "        if content_data['tables'] or content_data['images']:\n",
    "            print(f\"     â†’ Creating AI summary for mixed content...\")\n",
    "            try:\n",
    "                enhanced_content = create_ai_enhanced_summary(\n",
    "                    content_data['text'],\n",
    "                    content_data['tables'], \n",
    "                    content_data['images']\n",
    "                )\n",
    "                print(f\"     â†’ AI summary created successfully\")\n",
    "                print(f\"     â†’ Enhanced content preview: {enhanced_content[:200]}...\")\n",
    "            except Exception as e:\n",
    "                print(f\"      AI summary failed: {e}\")\n",
    "                enhanced_content = content_data['text']\n",
    "        else:\n",
    "            print(f\"     â†’ Using raw text (no tables/images)\")\n",
    "            enhanced_content = content_data['text']\n",
    "        \n",
    "        # Create LangChain Document with rich metadata\n",
    "        doc = Document(\n",
    "            page_content=enhanced_content,\n",
    "            metadata={\n",
    "                \"original_content\": json.dumps({\n",
    "                    \"raw_text\": content_data['text'],\n",
    "                    \"tables_html\": content_data['tables'],\n",
    "                    \"images_base64\": content_data['images']\n",
    "                })\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        langchain_documents.append(doc)\n",
    "    \n",
    "    print(f\" Processed {len(langchain_documents)} chunks\")\n",
    "    return langchain_documents\n",
    "\n",
    "\n",
    "# Process chunks with AI\n",
    "processed_chunks = summarise_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5a2c05",
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_chunks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d195220a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_chunks_to_json(chunks, filename=\"chunks_export.json\"):\n",
    "    \"\"\"Export processed chunks to clean JSON format\"\"\"\n",
    "    export_data = []\n",
    "    \n",
    "    for i, doc in enumerate(chunks):\n",
    "        chunk_data = {\n",
    "            \"chunk_id\": i + 1,\n",
    "            \"enhanced_content\": doc.page_content,\n",
    "            \"metadata\": {\n",
    "                \"original_content\": json.loads(doc.metadata.get(\"original_content\", \"{}\"))\n",
    "            }\n",
    "        }\n",
    "        export_data.append(chunk_data)\n",
    "    \n",
    "    # Save to file\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(export_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\" Exported {len(export_data)} chunks to {filename}\")\n",
    "    return export_data\n",
    "\n",
    "# Export your chunks\n",
    "json_data = export_chunks_to_json(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44d16f90",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_vector_store(documents, persist_directory=\"dbv1/chroma_db\"):\n",
    "    \"\"\"Create and persist ChromaDB vector store\"\"\"\n",
    "    print(\" Creating embeddings and storing in ChromaDB...\")\n",
    "        \n",
    "    embedding_model = OpenAIEmbeddings(model=\"text-embedding-3-small\")\n",
    "    \n",
    "    # Create ChromaDB vector store\n",
    "    print(\"--- Creating vector store ---\")\n",
    "    vectorstore = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embedding_model,\n",
    "        persist_directory=persist_directory, \n",
    "        collection_metadata={\"hnsw:space\": \"cosine\"}\n",
    "    )\n",
    "    print(\"--- Finished creating vector store ---\")\n",
    "    \n",
    "    print(f\" Vector store created and saved to {persist_directory}\")\n",
    "    return vectorstore\n",
    "\n",
    "# Create the vector store\n",
    "db = create_vector_store(processed_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28e54d37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After your retrieval\n",
    "query = \"What are the two main components of the Transformer architecture? \"\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "# Export to JSON\n",
    "export_chunks_to_json(chunks, \"rag_results.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d76db6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_complete_ingestion_pipeline(pdf_path: str):\n",
    "    \"\"\"Run the complete RAG ingestion pipeline\"\"\"\n",
    "    print(\"ðŸš€ Starting RAG Ingestion Pipeline\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Step 1: Partition\n",
    "    elements = partition_document(pdf_path)\n",
    "    \n",
    "    # Step 2: Chunk\n",
    "    chunks = create_chunks_by_title(elements)\n",
    "    \n",
    "    # Step 3: AI Summarisation\n",
    "    summarised_chunks = summarise_chunks(chunks)\n",
    "    \n",
    "    # Step 4: Vector Store\n",
    "    db = create_vector_store(summarised_chunks, persist_directory=\"dbv2/chroma_db\")\n",
    "    \n",
    "    print(\" Pipeline completed successfully!\")\n",
    "    return db\n",
    "\n",
    "# Run the complete pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23252ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "db = run_complete_ingestion_pipeline(\"./docs/attention-is-all-you-need.pdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e319d6e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Query the vector store\n",
    "query = \"What are the two main components of the Transformer architecture?\"\n",
    "\n",
    "retriever = db.as_retriever(search_kwargs={\"k\": 3})\n",
    "chunks = retriever.invoke(query)\n",
    "\n",
    "def generate_final_answer(chunks, query):\n",
    "    \"\"\"Generate final answer using multimodal content\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # Initialize LLM (needs vision model for images)\n",
    "        llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)\n",
    "        \n",
    "        # Build the text prompt\n",
    "        prompt_text = f\"\"\"Based on the following documents, please answer this question: {query}\n",
    "\n",
    "CONTENT TO ANALYZE:\n",
    "\"\"\"\n",
    "        \n",
    "        for i, chunk in enumerate(chunks):\n",
    "            prompt_text += f\"--- Document {i+1} ---\\n\"\n",
    "            \n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                \n",
    "                # Add raw text\n",
    "                raw_text = original_data.get(\"raw_text\", \"\")\n",
    "                if raw_text:\n",
    "                    prompt_text += f\"TEXT:\\n{raw_text}\\n\\n\"\n",
    "                \n",
    "                # Add tables as HTML\n",
    "                tables_html = original_data.get(\"tables_html\", [])\n",
    "                if tables_html:\n",
    "                    prompt_text += \"TABLES:\\n\"\n",
    "                    for j, table in enumerate(tables_html):\n",
    "                        prompt_text += f\"Table {j+1}:\\n{table}\\n\\n\"\n",
    "            \n",
    "            prompt_text += \"\\n\"\n",
    "        \n",
    "        prompt_text += \"\"\"\n",
    "Please provide a clear, comprehensive answer using the text, tables, and images above. If the documents don't contain sufficient information to answer the question, say \"I don't have enough information to answer that question based on the provided documents.\"\n",
    "\n",
    "ANSWER:\"\"\"\n",
    "\n",
    "        # Build message content starting with text\n",
    "        message_content = [{\"type\": \"text\", \"text\": prompt_text}]\n",
    "        \n",
    "        # Add all images from all chunks\n",
    "        for chunk in chunks:\n",
    "            if \"original_content\" in chunk.metadata:\n",
    "                original_data = json.loads(chunk.metadata[\"original_content\"])\n",
    "                images_base64 = original_data.get(\"images_base64\", [])\n",
    "                \n",
    "                for image_base64 in images_base64:\n",
    "                    message_content.append({\n",
    "                        \"type\": \"image_url\",\n",
    "                        \"image_url\": {\"url\": f\"data:image/jpeg;base64,{image_base64}\"}\n",
    "                    })\n",
    "        \n",
    "        # Send to AI and get response\n",
    "        message = HumanMessage(content=message_content)\n",
    "        response = llm.invoke([message])\n",
    "        \n",
    "        return response.content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\" Answer generation failed: {e}\")\n",
    "        return \"Sorry, I encountered an error while generating the answer.\"\n",
    "\n",
    "# Usage\n",
    "final_answer = generate_final_answer(chunks, query)\n",
    "print(final_answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
